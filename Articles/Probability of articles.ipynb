{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pobieramy potrzebne biblioteki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\grzeg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\grzeg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\grzeg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\grzeg\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download([\"punkt\", \"stopwords\", \"wordnet\", \"omw-1.4\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dodajemy wyrazy, które nie zmieniają sensu wypowiedzi jeśli je usuniemy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "stopwords.append(\"'s\")\n",
    "stopwords.append(\"'re\")\n",
    "stopwords.append(\",\")\n",
    "stopwords.append(\".\")\n",
    "stopwords.append(\"``\")\n",
    "stopwords.append(\"''\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otwieramy nasze artykuły i dokonujemy lematyzacji (usuwamy odmiany słów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_1 = open('Johanessburg.txt').read()\n",
    "f_2 = open('Canada.txt').read()\n",
    "f_3 = open('Kfc.txt').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = []\n",
    "\n",
    "article = f_1\n",
    "tokenized_article_word = word_tokenize(article)\n",
    "filtered_1=[]\n",
    "for w in tokenized_article_word:\n",
    "    if w not in stopwords:\n",
    "        filtered_1.append(w)\n",
    "\n",
    "lemmatized.append(' '.join(filtered_1))\n",
    "\n",
    "article = f_2\n",
    "tokenized_article_word = word_tokenize(article)\n",
    "filtered_2=[]\n",
    "for w in tokenized_article_word:\n",
    "    if w not in stopwords:\n",
    "        filtered_2.append(w)\n",
    "\n",
    "lemmatized.append(' '.join(filtered_2))\n",
    "\n",
    "article = f_3\n",
    "tokenized_article_word = word_tokenize(article)\n",
    "filtered_3=[]\n",
    "for w in tokenized_article_word:\n",
    "    if w not in stopwords:\n",
    "        filtered_3.append(w)\n",
    "\n",
    "lemmatized.append(' '.join(filtered_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zapisujemy bazę dokumentów jako wektory w macierzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Macierz DTM oznacza macierz bazy dokumentów, w której zliczane jest każde słowo w dokumencie. Wszystkie zliczenia są zapisywane w wektorze. Mając trzy artykuły otrzymujemy macierz z trzech wektorów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macierz DTM:\n",
      " [[2 1 1 ... 1 0 0]\n",
      " [2 0 0 ... 0 1 0]\n",
      " [0 1 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "corpus = lemmatized\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "X_array = X.toarray()\n",
    "print('Macierz DTM:\\n', X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tworzymy macierz TFIDF która pokazuje występowanie słowa podzielone przez liczbę wszystkich słów danego dokumentu pomnożone przez logarytm z liczby wszystkich dokumentów podzielonych przez liczbę dokumentów z tym słowem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macierz TFIDF:\n",
      "         000        10        11     125th        13        15        18  \\\n",
      "0  0.036194  0.018097  0.023795  0.023795  0.000000  0.000000  0.000000   \n",
      "1  0.091744  0.000000  0.000000  0.000000  0.060316  0.060316  0.000000   \n",
      "2  0.000000  0.032598  0.000000  0.000000  0.000000  0.000000  0.042862   \n",
      "\n",
      "       1886     1970s      1974  ...     wraps      year     years      york  \\\n",
      "0  0.023795  0.023795  0.023795  ...  0.000000  0.072388  0.014054  0.023795   \n",
      "1  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.035624  0.000000   \n",
      "2  0.000000  0.000000  0.000000  ...  0.042862  0.097793  0.025315  0.000000   \n",
      "\n",
      "       your     zoned       zoo      zulu     âł403       âł5  \n",
      "0  0.023795  0.023795  0.000000  0.023795  0.000000  0.000000  \n",
      "1  0.000000  0.000000  0.060316  0.000000  0.060316  0.000000  \n",
      "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.042862  \n",
      "\n",
      "[3 rows x 832 columns]\n"
     ]
    }
   ],
   "source": [
    "dt = pd.DataFrame(X_array)\n",
    "dt.columns = vectorizer.get_feature_names_out()\n",
    "\n",
    "dt.iloc[0, :] = dt.iloc[0, :] / np.count_nonzero(X_array[0])\n",
    "dt.iloc[1, :] = dt.iloc[1, :] / np.count_nonzero(X_array[1])\n",
    "dt.iloc[2, :] = dt.iloc[2, :] / np.count_nonzero(X_array[2])\n",
    "\n",
    "text_tf = TfidfVectorizer().fit_transform(corpus)\n",
    "\n",
    "tfidf = pd.DataFrame(text_tf.toarray())\n",
    "tfidf.columns = vectorizer.get_feature_names_out()\n",
    "print('Macierz TFIDF:\\n', tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obliczamy prawdopodobieństwo cosinusowe między artykułami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podobieństwo cosinusowe artykułów Johanessburg i Canada: 0.0580618448093488\n",
      "Podobieństwo cosinusowe artykułów Johanessburg i KFC: 0.11151437624878197\n",
      "Podobieństwo cosinusowe artykułów Canada i KFC: 0.04362652574218082\n"
     ]
    }
   ],
   "source": [
    "p1 = dot(dt.iloc[0, :], dt.iloc[1, :])/(norm(dt.iloc[0, :])*norm(dt.iloc[1, :]))\n",
    "print('Podobieństwo cosinusowe artykułów Johanessburg i Canada:', p1)\n",
    "p2 = dot(dt.iloc[0, :], dt.iloc[2, :])/(norm(dt.iloc[0, :])*norm(dt.iloc[2, :]))\n",
    "print('Podobieństwo cosinusowe artykułów Johanessburg i KFC:', p2)\n",
    "p3 = dot(dt.iloc[1, :], dt.iloc[2, :])/(norm(dt.iloc[1, :])*norm(dt.iloc[2, :]))\n",
    "print('Podobieństwo cosinusowe artykułów Canada i KFC:', p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wnioski: Najbardziej zbliżone do siebie są artykuły Johanessburg i KFC"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31bda736cc3a2b74657268fa5d3ad0c209626371abb9b0a361130e2a69d8dcd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
